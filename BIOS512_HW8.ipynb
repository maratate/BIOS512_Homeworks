{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77b97eda-a875-49d9-922a-aca98f18d1ea",
   "metadata": {},
   "source": [
    "# Homework 08\n",
    "This homework is based on the clustering lectures. Check the lecture notes and TA notes - they should help!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75ac688-c3c4-4916-a858-97104be7e273",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "This question will walk you through creating your own `kmeans` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3eefd-fd8c-4688-8b9e-e2368e56d526",
   "metadata": {},
   "source": [
    "#### a) What are the steps of `kmeans`?\n",
    "**Hint**: There are 4 steps/builder functions that you'll need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774766c3-f00f-413f-8e43-a898b2a4bf24",
   "metadata": {},
   "source": [
    "a) Answer: \n",
    "1.  Assign each point to a cluster N at random.\n",
    "2.  Calculate the mean position of each cluster using the previous\n",
    "    assignments.\n",
    "3.  Loop through the points - assign each point to the cluster to whose\n",
    "    center it is closest.\n",
    "4.  Repeat this process until the centers stop moving around."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8b199b-d1d9-4a55-ad95-9e86ad0655bd",
   "metadata": {},
   "source": [
    "#### b) Create the builder function for step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a92a92-dfd3-4cae-97d8-1a370bcc1033",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_randomly <- function(n_points, n_clusters) {\n",
    "  # assign cluster labels randomly\n",
    "  labels <- ((1:n_points - 1) %% n_clusters) + 1\n",
    "  sample(labels, n_points, replace = FALSE)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e44ddaf-4506-47d0-98fb-09871e08808c",
   "metadata": {},
   "source": [
    "#### c) Create the builder function for step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4f65a8-a945-4a1a-a841-54de8212a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cluster_means <- function(data, labels) {\n",
    "  data %>%\n",
    "    mutate(label = labels) %>%\n",
    "    group_by(label) %>%\n",
    "    summarise(across(everything(), mean), .groups = \"drop\") %>%\n",
    "    arrange(label)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8640c6-2d2f-49c3-b7ed-da0d4fb13935",
   "metadata": {},
   "source": [
    "#### d) Create the builder function for step 3.\n",
    "*Hint*: There are two ways to do this part - one is significantly more efficient than the other. You can do either.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aceae6-09d9-489a-bf9f-5a6ac0055bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_cluster <- function(data, means) {\n",
    "  data_mat <- as.matrix(data)\n",
    "  means_mat <- as.matrix(means %>% select(-label))\n",
    "  \n",
    "  dist_mat <- outer(\n",
    "    1:nrow(data_mat), 1:nrow(means_mat),\n",
    "    Vectorize(function(i, j) sum((data_mat[i, ] - means_mat[j, ])^2))\n",
    "  )\n",
    "      \n",
    "  labels <- means$label[apply(dist_mat, 1, which.min)]\n",
    "  labels\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbea660-9bd2-4dae-9a5c-838f613b0d7c",
   "metadata": {},
   "source": [
    "#### e) Create the builder function for step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbb9995-dcfc-4d31-b671-22b9522c9671",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_done <- function(old_means, new_means, eps = 1e-6) {\n",
    "  om <- as.matrix(old_means %>% select(-label))\n",
    "  nm <- as.matrix(new_means %>% select(-label))\n",
    "  \n",
    "  m <- mean(sqrt(rowSums((om - nm)^2)))\n",
    "  m < eps\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c86f61-68d2-484a-9c9c-db7f35c8e685",
   "metadata": {},
   "source": [
    "#### f) Combine them all into your own `kmeans` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb12d7a-6e85-42d2-b1e7-3ca6fa19b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mykmeans <- function(data, n_clusters, eps = 1e-6) {\n",
    "  labels <- label_randomly(nrow(data), n_clusters)\n",
    "  old_means <- get_cluster_means(data, labels)\n",
    "  done <- FALSE\n",
    "  \n",
    "  while (!done) {\n",
    "    labels <- assign_cluster(data, old_means)\n",
    "    new_means <- get_cluster_means(data, labels)\n",
    "    \n",
    "    if (kmeans_done(old_means, new_means, eps)) {\n",
    "      done <- TRUE\n",
    "    } else {\n",
    "      old_means <- new_means\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  list(labels = labels, means = new_means)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da13180d-3a51-4218-94a0-3f362612f099",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "This is when we'll test your `kmeans` function.\n",
    "#### a) Read in the `voltages_df.csv` data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd23409-dc81-446e-932b-cfe39dd1da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "voltages_df <- read_csv(\"~/Desktop/BIOS512/Homeworks/Homework 8/voltages_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f002a7b-eda1-4d2d-8ef9-3090b563708d",
   "metadata": {},
   "source": [
    "#### b) Call your `kmeans` function with 3 clusters. Print the results with `results$labels` and `results$means`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e99ed-30cb-4aad-81a8-4ac7893c8225",
   "metadata": {},
   "outputs": [],
   "source": [
    "results <- mykmeans(voltages_df, n_clusters = 3)\n",
    "print(results$labels)\n",
    "print(results$means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9949c-60ce-4a75-8688-e3c7485122ab",
   "metadata": {},
   "source": [
    "#### c) Call R's `kmeans` function with 3 clusters. Print the results with `results$labels` and `results$cluster`. \n",
    "*Hint*: Use the `as.matrix()` function to make the `voltages_df` data frame a matrix before calling `kmeans()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67941bc6-0a9b-4917-b534-488b874a84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "voltages_matrix <- as.matrix(voltages_df)\n",
    "results2 <- kmeans(voltages_matrix, centers = 3)\n",
    "\n",
    "print(results2$centers)   \n",
    "print(results2$cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4944ea46-ed01-413b-8b55-d55f383413de",
   "metadata": {},
   "source": [
    "#### d) Are your labels/clusters the same? If not, why? Are your means the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28604a6a-985a-415e-acbd-4f1dd813a259",
   "metadata": {},
   "source": [
    "Answer: No, the labels/clusters are not the same because cluster numbers are arbitrary and randomly assigned labels, not meaningful identifiers. Even if both algorithms work correctly, K-means is not guaranteed to assign the same label (e.g., cluster 1 vs. cluster 2) to the same group. Since my mykmeans function uses its own initialization logic, the order of cluster centers naturally differs from R’s kmeans output. The mean values are the same, but they are associated with different cluster numbers due to the random nature of label assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228e5ce-269f-4d83-b94e-40f3c4a137cd",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "#### a) Explain the process of using a for loop to assign clusters for kmeans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8dbd2d-c7e2-4df0-9a79-034df0c7a0ec",
   "metadata": {},
   "source": [
    "Answer: When using a for loop to assign clusters in kmeans, you loop over each data point (i) and, for each one, loop over all cluster centers (j) to calculate the distance between the point and each cluster mean. The point is then assigned to the cluster with the smallest distance. This process involves a double loop, one over all data points (N) and another over all clusters (K), which makes it computationally slow, especially for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2815b679-8bd1-4ec2-85d3-e42166f3e0ff",
   "metadata": {},
   "source": [
    "#### b) Explain the process of vectorizing the code to assign clusters for kmeans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daced3da-d6ae-4fb7-aef2-98aac7263dde",
   "metadata": {},
   "source": [
    "Answer: When vectorizing the code to assign clusters in kmeans, instead of looping through each point and cluster, you create two large replicated matrices so all point–cluster pairs can be compared simultaneously. The first matrix, Data′, replicates each data point K times (once for each cluster), and the second matrix, Means′, replicates each cluster mean N times (once for each data point). These matrices now have the same dimensions (N × K × D), allowing you to subtract them directly and compute all distances in a single vectorized operation. This approach eliminates explicit loops and greatly improves computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be7b85-c4f0-4626-b7f5-0ba921efcb6d",
   "metadata": {},
   "source": [
    "#### c) State which (for loops or vectorizing) is more efficient and why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e9dff3-704f-4131-b978-3e6e017d4fae",
   "metadata": {},
   "source": [
    "Answer: Vectorizing is more efficient because it eliminates the need for slow double loops over N and K. Instead of iterating through each data point and cluster, vectorization uses matrix operations. This allows the replicated matrices to be generated and all distances computed simultaneously, making the process much faster and more efficient than using explicit loops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2824d8-b8b6-4746-8fdc-8b3a8a1ca49d",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "#### When does `kmeans` fail? What assumption does `kmeans` use that causes it to fail in this situation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77fac83-154d-433e-a033-21e3fa604b92",
   "metadata": {},
   "source": [
    "Answer: Kmeans fails when the data are not well separated, not spherical, or when clusters differ greatly in size or density. Kmeans assumes that the data are vector-based and that clusters are spherical (uniformly shaped Gaussians) of roughly equal size and variance. It also assumes that each point belongs to exactly one cluster. These assumptions cause Kmeans to perform poorly when clusters overlap, have irregular shapes, or vary in size and density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c5a711-449c-48e0-b02b-8ad3346e8d38",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "#### What assumption do Guassian mixture models make?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b27c9-e4e5-4a52-acd5-0c06fe324c3c",
   "metadata": {},
   "source": [
    "Answer: A Gaussian mixture model assumes that the data is drawn from a mixture of N Gaussian (normal) distributions, each with its own mean and covariance. These distributions represent different clusters, and their parameters  are estimated from the data. This assumption allows GMMs to model clusters of varying sizes, shapes, and orientations more flexibly than kmeans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a4f52-f2d5-45d7-aeb2-a05ccab9e2fe",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "#### What assumption does spectral clustering make? Why does this help us?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c019842-99f4-4176-a86b-52ae63863de3",
   "metadata": {},
   "source": [
    "Answer: Spectral clustering makes the assumption that two points are more likely to be in the same cluster if they are close to one another. The mere existence of a metric is a much weaker condition than the existence of a vector space, and thus we can work with substantially more types of data and complex data structures. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede993cf-222f-4c5b-b09e-bc2c988a777e",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "#### Define the gap statistic method. What do we use it for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51412b9a-99cc-4916-8755-b3049d358069",
   "metadata": {},
   "source": [
    "Answer: The gap statistic method is a standard method of calculating the cluster number. It compares the clustering for each value of K to a cluster of data \"randomized\" into the same domain as the original data. Then it compute the dispersion of the two clusterings and look at the difference. The “gap” is the difference between the observed and expected dispersion. The optimal number of clusters is typically chosen at the point where this gap is largest (the “knee”), indicating that the clustering structure better than what would be expected by chance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
